{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection Pipeline\n",
    "This notebook contains the consolidated fraud detection pipeline from Weeks 5-7.\n",
    "It loads transaction data, engineers features, trains XGBoost, and logs to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Cell 1: Imports\n",
    "# =============================================================================\n",
    "# We import the core libraries needed for our fraud detection pipeline:\n",
    "# - pandas/numpy for data manipulation\n",
    "# - xgboost for our gradient boosting classifier\n",
    "# - sklearn for evaluation metrics and data splitting\n",
    "# - mlflow for experiment tracking and model logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings to keep notebook output clean during demos\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All imports successful!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Cell 2: Load Data\n",
    "# =============================================================================\n",
    "# Load the transaction dataset from our local data directory.\n",
    "# This CSV contains simulated credit card transactions with a binary\n",
    "# 'is_fraud' label. We print basic stats to sanity-check the data.\n",
    "\n",
    "df = pd.read_csv('../data/transactions_sample.csv')\n",
    "\n",
    "# Print the shape so we know how many rows and columns we have\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Print column names to understand what features are available\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Print the fraud rate - this tells us how imbalanced the dataset is.\n",
    "# Fraud detection is almost always highly imbalanced (fraud is rare),\n",
    "# which affects our choice of metrics and training strategy.\n",
    "fraud_rate = df['is_fraud'].mean()\n",
    "print(f\"Fraud rate: {fraud_rate:.4f} ({fraud_rate*100:.2f}%)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Cell 3: Explore Data\n",
    "# =============================================================================\n",
    "# Before building features, we explore the raw data to understand its\n",
    "# structure, data types, and any quality issues like missing values.\n",
    "\n",
    "# Preview the first few rows to see what the data looks like\n",
    "print(\"=== First 5 Rows ===\")\n",
    "print(df.head())\n",
    "print()\n",
    "\n",
    "# Check shape again for confirmation\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print()\n",
    "\n",
    "# Data types help us understand which columns are numeric vs categorical\n",
    "print(\"=== Data Types ===\")\n",
    "print(df.dtypes)\n",
    "print()\n",
    "\n",
    "# Missing values can cause model training to fail or produce biased results\n",
    "print(\"=== Missing Values ===\")\n",
    "print(df.isnull().sum())\n",
    "print()\n",
    "\n",
    "# Fraud distribution shows us the class balance\n",
    "print(\"=== Fraud Distribution ===\")\n",
    "print(df['is_fraud'].value_counts())\n",
    "print()\n",
    "\n",
    "# Compare transaction amounts between fraud and non-fraud\n",
    "# Fraudulent transactions often have different amount patterns\n",
    "print(\"=== Amount by Fraud Status ===\")\n",
    "print(df.groupby('is_fraud')['amount'].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# Cell 4: Time-Based Feature Engineering\n# =============================================================================\n# Fraud patterns often depend on WHEN a transaction occurs.\n# Transactions at 3am on a Sunday are more suspicious than at noon on Tuesday.\n# We create several time-based features to capture these patterns.\n\n# Our dataset already has 'hour' (0-23) and 'day_of_week' (0=Mon, 6=Sun).\n# We use these to create binary and cyclical features.\n\n# Is this a weekend transaction? Fraud patterns differ on weekends.\n# day_of_week: Monday=0, Sunday=6, so >= 5 means Saturday or Sunday\ndf['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n\n# Is this a nighttime transaction? (between 10pm and 6am)\n# Night transactions are statistically more likely to be fraudulent\ndf['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)\n\n# Cyclical encoding of hour using sin/cos.\n# Why? Because hour 23 and hour 0 are close in time but far apart numerically.\n# Sin/cos encoding preserves this cyclical relationship.\ndf['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\ndf['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n\n# Cyclical encoding of day of week (same reasoning as hour)\ndf['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\ndf['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n\nprint(\"Time features created:\")\nprint(f\"  is_weekend: {df['is_weekend'].value_counts().to_dict()}\")\nprint(f\"  is_night:   {df['is_night'].value_counts().to_dict()}\")\nprint(f\"  hour_sin range: [{df['hour_sin'].min():.3f}, {df['hour_sin'].max():.3f}]\")\nprint(f\"  hour_cos range: [{df['hour_cos'].min():.3f}, {df['hour_cos'].max():.3f}]\")\nprint(f\"  day_sin range:  [{df['day_sin'].min():.3f}, {df['day_sin'].max():.3f}]\")\nprint(f\"  day_cos range:  [{df['day_cos'].min():.3f}, {df['day_cos'].max():.3f}]\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Cell 5: Amount-Based Feature Engineering\n",
    "# =============================================================================\n",
    "# Transaction amount is one of the most important features for fraud detection.\n",
    "# Raw amounts can be skewed, so we create transformed versions.\n",
    "\n",
    "# Log transform of amount using log1p (log(1+x) to handle zeros safely).\n",
    "# This reduces the impact of extreme values and makes the distribution\n",
    "# more normal, which helps tree-based models find better splits.\n",
    "df['amount_log'] = np.log1p(df['amount'])\n",
    "\n",
    "# Z-score normalization: how many standard deviations from the mean?\n",
    "# Transactions with very high z-scores are unusual and potentially fraudulent.\n",
    "df['amount_zscore'] = (df['amount'] - df['amount'].mean()) / df['amount'].std()\n",
    "\n",
    "# Percentile rank: what fraction of transactions have a lower amount?\n",
    "# A percentile of 0.99 means the transaction is larger than 99% of all transactions.\n",
    "df['amount_percentile'] = df['amount'].rank(pct=True)\n",
    "\n",
    "print(\"Amount features created:\")\n",
    "print(f\"  amount_log range:        [{df['amount_log'].min():.3f}, {df['amount_log'].max():.3f}]\")\n",
    "print(f\"  amount_zscore range:     [{df['amount_zscore'].min():.3f}, {df['amount_zscore'].max():.3f}]\")\n",
    "print(f\"  amount_percentile range: [{df['amount_percentile'].min():.3f}, {df['amount_percentile'].max():.3f}]\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Cell 6: Train/Test Split\n",
    "# =============================================================================\n",
    "# We split our data into training and test sets BEFORE training.\n",
    "# This prevents data leakage - the model never sees test data during training.\n",
    "\n",
    "# Define the feature columns we'll use for training.\n",
    "# These are all the engineered features we created above.\n",
    "feature_cols = [\n",
    "    'amount', 'amount_log', 'amount_zscore', 'amount_percentile',\n",
    "    'is_weekend', 'is_night',\n",
    "    'hour_sin', 'hour_cos', 'day_sin', 'day_cos'\n",
    "]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['is_fraud']\n",
    "\n",
    "# Stratified split ensures both train and test sets have the same fraud rate.\n",
    "# This is critical for imbalanced datasets - without stratification, the test\n",
    "# set might have no fraud cases at all!\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size:     {X_test.shape[0]} samples\")\n",
    "print(f\"Training fraud rate: {y_train.mean():.4f}\")\n",
    "print(f\"Test fraud rate:     {y_test.mean():.4f}\")\n",
    "print(f\"Number of features:  {len(feature_cols)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Cell 7: Train XGBoost Model\n",
    "# =============================================================================\n",
    "# XGBoost is an excellent choice for tabular fraud detection because:\n",
    "# - It handles imbalanced classes well (via scale_pos_weight)\n",
    "# - It captures non-linear feature interactions automatically\n",
    "# - It's fast to train and highly accurate on structured data\n",
    "\n",
    "# Define hyperparameters for our XGBoost classifier\n",
    "params = {\n",
    "    'max_depth': 6,              # Maximum tree depth - controls model complexity\n",
    "    'n_estimators': 100,         # Number of boosting rounds (trees)\n",
    "    'learning_rate': 0.1,        # Step size shrinkage - prevents overfitting\n",
    "    'eval_metric': 'logloss',    # Binary cross-entropy loss for classification\n",
    "    'use_label_encoder': False,  # Suppress deprecation warning\n",
    "    'random_state': 42           # Reproducibility\n",
    "}\n",
    "\n",
    "# Create and train the model\n",
    "model = xgb.XGBClassifier(**params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "print(f\"Parameters: {params}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Cell 8: Evaluate Model\n",
    "# =============================================================================\n",
    "# For fraud detection, accuracy alone is misleading (a model that predicts\n",
    "# \"no fraud\" for everything gets 99%+ accuracy on imbalanced data).\n",
    "# We use multiple metrics to get a complete picture:\n",
    "# - Precision: Of predicted frauds, how many are real? (false alarm rate)\n",
    "# - Recall: Of actual frauds, how many did we catch? (miss rate)\n",
    "# - F1: Harmonic mean of precision and recall\n",
    "# - ROC-AUC: Overall ranking quality of the model\n",
    "\n",
    "# Generate predictions on the held-out test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of fraud class\n",
    "\n",
    "# Calculate all evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"=== Model Evaluation ===\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "print()\n",
    "\n",
    "# Confusion matrix shows the breakdown of correct/incorrect predictions\n",
    "# Format: [[True Neg, False Pos], [False Neg, True Pos]]\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(f\"  True Negatives:  {cm[0][0]}\")\n",
    "print(f\"  False Positives: {cm[0][1]}\")\n",
    "print(f\"  False Negatives: {cm[1][0]}\")\n",
    "print(f\"  True Positives:  {cm[1][1]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Cell 9: MLflow Logging\n",
    "# =============================================================================\n",
    "# MLflow tracks our experiments so we can compare different model versions.\n",
    "# We log parameters (what we configured), metrics (how well it performed),\n",
    "# and the model itself (so we can reload it later for serving).\n",
    "\n",
    "# Set the experiment name - this groups related runs together\n",
    "mlflow.set_experiment(\"fraud-detection\")\n",
    "\n",
    "# Start a new run and log everything\n",
    "with mlflow.start_run(run_name=\"xgboost-baseline\"):\n",
    "    # Log hyperparameters so we know exactly how this model was configured\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # Log evaluation metrics so we can compare across runs\n",
    "    mlflow.log_metrics({\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    })\n",
    "\n",
    "    # Log the trained model artifact - this saves the model so it can be\n",
    "    # loaded later for inference or deployed to a serving endpoint\n",
    "    mlflow.xgboost.log_model(model, \"model\")\n",
    "\n",
    "    print(\"MLflow logging complete!\")\n",
    "    print(f\"  Experiment: fraud-detection\")\n",
    "    print(f\"  Run name:   xgboost-baseline\")\n",
    "    print(f\"  Params logged: {len(params)}\")\n",
    "    print(f\"  Metrics logged: 5\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Cell 10: Feature Importance\n",
    "# =============================================================================\n",
    "# Understanding which features matter most helps us:\n",
    "# - Explain the model to stakeholders (\"amount is the top predictor\")\n",
    "# - Identify features to keep or remove in future iterations\n",
    "# - Validate that the model learned sensible patterns\n",
    "\n",
    "# Extract feature importances from the trained XGBoost model\n",
    "# These represent how much each feature contributed to reducing prediction error\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_\n",
    "})\n",
    "\n",
    "# Sort by importance descending so the most important features are at the top\n",
    "importance_df = importance_df.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"=== Feature Importance (sorted) ===\")\n",
    "for idx, row in importance_df.iterrows():\n",
    "    # Print a simple bar chart using characters for visual clarity\n",
    "    bar = '#' * int(row['importance'] * 50)\n",
    "    print(f\"  {row['feature']:25s} {row['importance']:.4f} {bar}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}